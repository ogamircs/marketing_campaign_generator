{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Marketing Campaign Creatives Generation Agent (100% Open-Source)\n\nInteractive LangGraph-based AI Agent for Marketing Content Generation using **state-of-the-art open-source models** - no API keys required!\n\nThis notebook implements an intelligent agent that:\n- Takes short user inputs and expands them into detailed prompts using **Qwen2.5-7B-Instruct** (local)\n- Generates promotional images using **FLUX.1-dev** (Black Forest Labs) - Best quality open-source image model\n- Generates promotional videos using **LTX-Video** (Lightricks) - Fast, real-time video generation\n- Generates voice-over narration using **Kokoro-82M** TTS (Hugging Face)\n- Allows iterative feedback and refinement\n\n**Requirements:**\n- Google Colab with A100 GPU (40GB VRAM)\n- No API keys needed - everything runs locally!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing the Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install Python dependencies\n!pip install -q -U \\\n    langgraph \\\n    langchain-core \\\n    diffusers \\\n    transformers \\\n    accelerate \\\n    torch \\\n    torchao \\\n    safetensors \\\n    sentencepiece \\\n    moviepy \\\n    soundfile \\\n    imageio-ffmpeg \\\n    bitsandbytes \\\n    kokoro>=0.9.2\n\n# Install system dependencies for TTS and video processing\n!apt-get -qq -y install espeak-ng ffmpeg"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** After running the above cell, restart the runtime and run all cells sequentially from the next cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Transformers (for local LLM)\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\n# LangGraph Libraries\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.graph.message import add_messages\n\n# LangChain Core\nfrom langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n\n# Type Annotations\nfrom typing import TypedDict, Annotated, Literal, Optional, List, Any\n\n# PyTorch and utilities\nimport torch\nimport gc\nimport os\nimport sys\nimport time\nimport matplotlib.pyplot as plt"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Client Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Model Configuration - ALL LOCAL, NO API KEYS!\nLLM_MODEL = \"Qwen/Qwen2.5-7B-Instruct\"  # Agent LLM - excellent for instruction following\nIMAGE_MODEL = \"black-forest-labs/FLUX.1-dev\"  # State-of-the-art image generation\nVIDEO_MODEL = \"Lightricks/LTX-Video\"  # Fast real-time video generation\nTTS_MODEL = \"hexgrad/Kokoro-82M\"  # Hugging Face TTS model\n\n# Generation Configuration\nMAX_ITERATIONS = 10  # Maximum feedback iterations\nIMAGE_WIDTH = 1024  # FLUX.1 output width\nIMAGE_HEIGHT = 1024  # FLUX.1 output height\nIMAGE_STEPS = 28  # FLUX.1-dev recommended steps (20-50)\nVIDEO_WIDTH = 768  # LTX-Video width (must be divisible by 32)\nVIDEO_HEIGHT = 512  # LTX-Video height (must be divisible by 32)\nVIDEO_FRAMES = 241  # LTX-Video frames (must be divisible by 8 + 1, e.g., 241 = 30*8+1 ~10 sec at 24fps)\nVIDEO_FPS = 24  # LTX-Video outputs at 24 FPS\nDEFAULT_VOICE = \"af_heart\"  # American Female (Heart) voice for Kokoro\n\n# LLM Generation Settings\nLLM_MAX_NEW_TOKENS = 1024\nLLM_TEMPERATURE = 0.7\n\n# Global variables for pre-loaded models\nllm_model = None\nllm_tokenizer = None\n\nprint(f\"Configuration loaded - 100% LOCAL, NO API KEYS!\")\nprint(f\"  - Agent LLM: {LLM_MODEL}\")\nprint(f\"  - Image Model: {IMAGE_MODEL} (FLUX.1-dev)\")\nprint(f\"  - Video Model: {VIDEO_MODEL} (LTX-Video)\")\nprint(f\"  - TTS Model: {TTS_MODEL}\")\nprint(f\"  - Video Duration: ~{VIDEO_FRAMES/VIDEO_FPS:.1f} seconds ({VIDEO_FRAMES} frames at {VIDEO_FPS} FPS)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Management Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_gpu_memory():\n",
    "    \"\"\"Clear GPU memory between model loads.\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        print(f\"  GPU memory cleared. Available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "\n",
    "def get_gpu_memory_usage():\n",
    "    \"\"\"Get current GPU memory usage.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        reserved = torch.cuda.memory_reserved() / 1e9\n",
    "        return f\"Allocated: {allocated:.2f} GB, Reserved: {reserved:.2f} GB\"\n",
    "    return \"GPU not available\"\n",
    "\n",
    "\n",
    "# Check GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Total Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. Video generation will be very slow or may fail.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Pre-Download and Cache Models\n\n**Important:** Run this cell to download all models before starting the agent. This ensures no download delays during the interactive session. Models are cached locally after first download.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# PRE-DOWNLOAD ALL MODELS\n# ============================================================\n# This cell downloads and caches all models so they're ready\n# when the agent starts. No network delays during generation!\n# ============================================================\n\nprint(\"=\"*60)\nprint(\"   DOWNLOADING AND CACHING MODELS\")\nprint(\"   This may take 15-30 minutes on first run...\")\nprint(\"   (FLUX.1-dev and LTX-Video are large models)\")\nprint(\"=\"*60)\n\n# --- 0. Download Qwen2.5-7B-Instruct (Agent LLM) ---\nprint(\"\\n[0/4] Downloading Qwen2.5-7B-Instruct for agent reasoning...\")\nprint(f\"      Model: {LLM_MODEL}\")\nprint(\"      Using 4-bit quantization to save memory...\")\ntry:\n    # 4-bit quantization config for memory efficiency\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.float16,\n        bnb_4bit_use_double_quant=True\n    )\n\n    llm_tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL)\n    llm_model = AutoModelForCausalLM.from_pretrained(\n        LLM_MODEL,\n        quantization_config=bnb_config,\n        device_map=\"auto\",\n        trust_remote_code=True\n    )\n    print(\"      Qwen2.5-7B-Instruct loaded and ready!\")\n    print(f\"      {get_gpu_memory_usage()}\")\nexcept Exception as e:\n    print(f\"      Error loading LLM: {e}\")\n    llm_model = None\n    llm_tokenizer = None\n\n# --- 1. Download FLUX.1-dev (Image Generation) ---\nprint(\"\\n[1/4] Downloading FLUX.1-dev for image generation...\")\nprint(f\"      Model: {IMAGE_MODEL}\")\nprint(\"      (This is a large model ~12GB, may take a few minutes)\")\ntry:\n    from diffusers import FluxPipeline\n\n    image_pipe = FluxPipeline.from_pretrained(\n        IMAGE_MODEL,\n        torch_dtype=torch.bfloat16  # FLUX works best with bfloat16\n    )\n    # Keep on CPU initially to save memory\n    print(\"      FLUX.1-dev loaded and ready!\")\n    print(f\"      {get_gpu_memory_usage()}\")\nexcept Exception as e:\n    print(f\"      Error loading FLUX.1-dev: {e}\")\n    image_pipe = None\n\n# --- 2. Download Kokoro TTS (Voice-over) ---\nprint(\"\\n[2/4] Downloading Kokoro TTS for voice-over...\")\nprint(f\"      Model: {TTS_MODEL}\")\ntry:\n    from kokoro import KPipeline\n    tts_pipe = KPipeline(lang_code='a')  # 'a' for American English\n    print(\"      Kokoro TTS loaded and ready!\")\nexcept Exception as e:\n    print(f\"      Error loading Kokoro TTS: {e}\")\n    tts_pipe = None\n\n# Clear GPU memory before loading the video model\nclear_gpu_memory()\n\n# --- 3. Download LTX-Video (Video Generation) ---\nprint(\"\\n[3/4] Downloading LTX-Video for video generation...\")\nprint(f\"      Model: {VIDEO_MODEL}\")\nprint(\"      (Fast real-time video generation model)\")\ntry:\n    from diffusers import LTXPipeline\n\n    video_pipe = LTXPipeline.from_pretrained(\n        VIDEO_MODEL,\n        torch_dtype=torch.bfloat16  # LTX-Video works best with bfloat16\n    )\n    # Keep on CPU initially to save memory\n    print(\"      LTX-Video loaded and ready!\")\nexcept Exception as e:\n    print(f\"      Error loading LTX-Video: {e}\")\n    video_pipe = None\n\n# Clear some GPU memory\nclear_gpu_memory()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"   ALL MODELS DOWNLOADED AND CACHED!\")\nprint(\"   You can now run the agent without download delays.\")\nprint(\"=\"*60)\nprint(f\"\\nFinal GPU state: {get_gpu_memory_usage()}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent State Schema Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    State schema for the marketing content generation agent.\n",
    "    Extended with voice-over support.\n",
    "    \"\"\"\n",
    "    # Message history\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "\n",
    "    # User inputs\n",
    "    user_input: str\n",
    "    feedback: Optional[str]\n",
    "\n",
    "    # Expanded prompts\n",
    "    expanded_image_prompt: Optional[str]\n",
    "    expanded_video_prompt: Optional[str]\n",
    "    expanded_voiceover_script: Optional[str]  # NEW: Script for TTS\n",
    "\n",
    "    # Generation configuration\n",
    "    generation_mode: Optional[Literal[\"image\", \"video\", \"both\"]]\n",
    "    include_voiceover: bool  # NEW: Whether to generate voice-over\n",
    "    voice_style: Optional[str]  # NEW: Kokoro voice style\n",
    "\n",
    "    # Generated outputs\n",
    "    generated_image: Optional[Any]  # PIL Image object\n",
    "    generated_video_path: Optional[str]\n",
    "    generated_audio_path: Optional[str]  # NEW: Path to voice-over audio\n",
    "    final_video_path: Optional[str]  # NEW: Video with audio combined\n",
    "\n",
    "    # Control flow\n",
    "    iteration_count: int\n",
    "    is_satisfied: bool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Tool 1: Expand Prompt (Qwen2.5-7B Local)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def parse_expanded_prompts(response_text: str) -> tuple:\n    \"\"\"\n    Parse the response from LLM into separate prompts.\n    Returns: (image_prompt, video_prompt, voiceover_script)\n    \"\"\"\n    image_prompt = \"\"\n    video_prompt = \"\"\n    voiceover_script = \"\"\n\n    # Split by markers\n    if \"IMAGE_PROMPT:\" in response_text:\n        parts = response_text.split(\"VIDEO_PROMPT:\")\n        image_prompt = parts[0].replace(\"IMAGE_PROMPT:\", \"\").strip()\n\n        if len(parts) > 1:\n            video_parts = parts[1].split(\"VOICEOVER_SCRIPT:\")\n            video_prompt = video_parts[0].strip()\n\n            if len(video_parts) > 1:\n                voiceover_script = video_parts[1].strip()\n\n    return image_prompt, video_prompt, voiceover_script\n\n\ndef generate_llm_response(messages: list) -> str:\n    \"\"\"\n    Generate a response using the local Qwen model.\n    Uses the global llm_model and llm_tokenizer.\n    \"\"\"\n    global llm_model, llm_tokenizer\n\n    if llm_model is None or llm_tokenizer is None:\n        raise RuntimeError(\"LLM model not loaded. Run the model pre-download cell first.\")\n\n    # Format messages for Qwen chat template\n    text = llm_tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True\n    )\n\n    # Tokenize\n    model_inputs = llm_tokenizer([text], return_tensors=\"pt\").to(llm_model.device)\n\n    # Generate\n    with torch.no_grad():\n        generated_ids = llm_model.generate(\n            **model_inputs,\n            max_new_tokens=LLM_MAX_NEW_TOKENS,\n            temperature=LLM_TEMPERATURE,\n            do_sample=True,\n            pad_token_id=llm_tokenizer.eos_token_id\n        )\n\n    # Decode only the new tokens\n    generated_ids = [\n        output_ids[len(input_ids):]\n        for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n    ]\n\n    response = llm_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n    return response\n\n\ndef expand_prompt(state: AgentState) -> dict:\n    \"\"\"\n    Uses Qwen2.5-7B (local) to expand a short user input into detailed prompts\n    for image generation, video generation, and voice-over.\n    \"\"\"\n    user_input = state[\"user_input\"]\n\n    system_prompt = \"\"\"You are an expert at creating detailed prompts for AI image and video generation.\nGiven a short description of promotional material needed, create THREE detailed prompts:\n\n1. IMAGE PROMPT: A detailed description for generating a promotional poster/image.\n   Include: visual style, colors, composition, mood, and specific details.\n   NOTE: Do NOT request text in the image - AI image models struggle with text rendering.\n   Focus on visual elements, products, atmosphere, and branding colors.\n\n2. VIDEO PROMPT: A detailed description for generating a 6-second promotional video.\n   Include: scene descriptions, camera movements, pacing, mood.\n   CRITICAL RULE: Do NOT include any people, humans, faces, hands, baristas, customers,\n   or any human figures in the video prompt. Focus ONLY on:\n   - Products (food, drinks, items)\n   - Environment/atmosphere (interior, exterior, decor)\n   - Objects and textures (cups, tables, steam, etc.)\n   - Abstract visuals and motion\n\n3. VOICEOVER SCRIPT: A short, punchy voice-over script (max 40 words) for the promotional video.\n   Include: tagline, key selling point, call-to-action.\n   Keep it natural and conversational.\n\nFormat your response EXACTLY as:\nIMAGE_PROMPT: [detailed image prompt here]\nVIDEO_PROMPT: [detailed video prompt here - NO PEOPLE]\nVOICEOVER_SCRIPT: [short script for TTS]\n\"\"\"\n\n    print(\"\\nExpanding prompts with Qwen2.5-7B...\")\n\n    try:\n        messages = [\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": f\"User request: {user_input}\"}\n        ]\n\n        response_text = generate_llm_response(messages)\n        image_prompt, video_prompt, voiceover_script = parse_expanded_prompts(response_text)\n\n        print(\"\\n\" + \"-\"*50)\n        print(\"EXPANDED PROMPTS\")\n        print(\"-\"*50)\n        print(f\"\\nImage Prompt Preview:\\n{image_prompt[:200]}...\" if len(image_prompt) > 200 else f\"\\nImage Prompt:\\n{image_prompt}\")\n        print(f\"\\nVideo Prompt Preview:\\n{video_prompt[:200]}...\" if len(video_prompt) > 200 else f\"\\nVideo Prompt:\\n{video_prompt}\")\n        print(f\"\\nVoiceover Script:\\n{voiceover_script}\")\n\n        return {\n            \"expanded_image_prompt\": image_prompt,\n            \"expanded_video_prompt\": video_prompt,\n            \"expanded_voiceover_script\": voiceover_script,\n            \"messages\": [AIMessage(content=\"Expanded your request into detailed prompts.\")]\n        }\n\n    except Exception as e:\n        print(f\"Error expanding prompts: {str(e)}\")\n        return {\n            \"messages\": [AIMessage(content=f\"Error expanding prompts: {str(e)}\")]\n        }"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Tool 2: Generate Poster (FLUX.1-dev)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def generate_poster(state: AgentState) -> dict:\n    \"\"\"\n    Generates a promotional image using pre-loaded FLUX.1-dev.\n    Uses the global image_pipe loaded during initialization.\n    FLUX.1-dev produces state-of-the-art image quality.\n    \"\"\"\n    global image_pipe\n\n    prompt = state.get(\"expanded_image_prompt\")\n    iteration = state.get(\"iteration_count\", 0)\n\n    if not prompt:\n        print(\"[ERROR] No image prompt available.\")\n        return {\"messages\": [AIMessage(content=\"Error: No image prompt available.\")]}\n\n    print(f\"\\n[IMAGE] Using FLUX.1-dev (state-of-the-art quality)...\")\n    print(f\"  {get_gpu_memory_usage()}\")\n\n    try:\n        # Check if pipeline is loaded\n        if image_pipe is None:\n            print(\"  [IMAGE] Pipeline not loaded, loading now...\")\n            from diffusers import FluxPipeline\n            image_pipe = FluxPipeline.from_pretrained(\n                IMAGE_MODEL,\n                torch_dtype=torch.bfloat16\n            )\n\n        # Move to GPU\n        image_pipe.to(\"cuda\")\n        print(f\"  [IMAGE] Model moved to GPU\")\n        print(f\"  {get_gpu_memory_usage()}\")\n\n        print(f\"[IMAGE] Generating image with FLUX.1-dev...\")\n        print(f\"  Resolution: {IMAGE_WIDTH}x{IMAGE_HEIGHT}\")\n        print(f\"  Steps: {IMAGE_STEPS}\")\n        print(f\"  Prompt: {prompt[:100]}...\")\n\n        # FLUX.1-dev generation\n        result = image_pipe(\n            prompt=prompt,\n            height=IMAGE_HEIGHT,\n            width=IMAGE_WIDTH,\n            num_inference_steps=IMAGE_STEPS,\n            guidance_scale=3.5,  # FLUX.1-dev works well with 3-4\n            generator=torch.Generator(device=\"cuda\").manual_seed(42)\n        )\n        image = result.images[0]\n\n        # Save image\n        image_path = f\"promotional_image_v{iteration + 1}.png\"\n        image.save(image_path)\n\n        # Move back to CPU to free GPU memory for other models\n        image_pipe.to(\"cpu\")\n        clear_gpu_memory()\n\n        # Verify file\n        import os\n        if os.path.exists(image_path):\n            file_size = os.path.getsize(image_path)\n            print(f\"[IMAGE] SUCCESS! Image saved: {image_path} ({file_size} bytes)\")\n        else:\n            print(f\"[ERROR] Failed to save image file\")\n\n        return {\n            \"generated_image\": image,\n            \"messages\": [AIMessage(content=f\"Image generated: {image_path}\")]\n        }\n\n    except Exception as e:\n        import traceback\n        if image_pipe is not None:\n            try:\n                image_pipe.to(\"cpu\")\n            except:\n                pass\n        clear_gpu_memory()\n        print(f\"[ERROR] Image generation failed: {str(e)}\")\n        traceback.print_exc()\n        return {\"messages\": [AIMessage(content=f\"Image error: {str(e)}\")]}"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool 3: Generate Voice Over (Kokoro TTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def generate_voiceover(state: AgentState) -> dict:\n    \"\"\"\n    Generates voice-over audio using pre-loaded Kokoro-82M TTS.\n    Uses the global tts_pipe loaded during initialization.\n    Saves audio as WAV file for debugging.\n    \"\"\"\n    global tts_pipe\n    import soundfile as sf\n    import numpy as np\n\n    script = state.get(\"expanded_voiceover_script\")\n    iteration = state.get(\"iteration_count\", 0)\n    voice_style = state.get(\"voice_style\", DEFAULT_VOICE)\n    include_vo = state.get(\"include_voiceover\", False)\n\n    print(f\"\\n[DEBUG] generate_voiceover called:\")\n    print(f\"  - include_voiceover: {include_vo}\")\n    print(f\"  - script available: {bool(script)}\")\n    print(f\"  - voice_style: {voice_style}\")\n\n    if not script:\n        print(\"[ERROR] No voiceover script available.\")\n        return {\n            \"generated_audio_path\": None,\n            \"messages\": [AIMessage(content=\"Error: No voiceover script available.\")]\n        }\n\n    if not include_vo:\n        print(\"[INFO] Voiceover skipped (not requested).\")\n        return {\n            \"generated_audio_path\": None,\n            \"messages\": [AIMessage(content=\"Voiceover skipped.\")]\n        }\n\n    print(f\"\\n[TTS] Using pre-loaded Kokoro TTS...\")\n\n    try:\n        # Check if pipeline is loaded\n        if tts_pipe is None:\n            print(\"  [TTS] Pipeline not loaded, loading now...\")\n            from kokoro import KPipeline\n            tts_pipe = KPipeline(lang_code='a')\n\n        print(f\"[TTS] Generating voiceover with voice: {voice_style}\")\n        print(f\"[TTS] Script: '{script}'\")\n\n        # Generate audio\n        generator = tts_pipe(\n            script,\n            voice=voice_style,\n            speed=1.0\n        )\n\n        # Collect all audio chunks\n        audio_chunks = []\n        sample_rate = 24000  # Kokoro uses 24kHz\n\n        for i, (gs, ps, audio) in enumerate(generator):\n            audio_chunks.append(audio)\n            print(f\"  [TTS] Generated chunk {i+1}, shape: {audio.shape}\")\n\n        if not audio_chunks:\n            print(\"[ERROR] No audio chunks generated!\")\n            return {\n                \"generated_audio_path\": None,\n                \"messages\": [AIMessage(content=\"Error: TTS generated no audio.\")]\n            }\n\n        # Concatenate audio\n        full_audio = np.concatenate(audio_chunks)\n        print(f\"[TTS] Full audio shape: {full_audio.shape}, duration: {len(full_audio)/sample_rate:.2f}s\")\n\n        # Save audio as WAV\n        audio_path = f\"voiceover_v{iteration + 1}.wav\"\n        sf.write(audio_path, full_audio, sample_rate)\n\n        # Verify file was saved\n        import os\n        if os.path.exists(audio_path):\n            file_size = os.path.getsize(audio_path)\n            print(f\"[TTS] SUCCESS! Voiceover saved: {audio_path} ({file_size} bytes)\")\n        else:\n            print(f\"[ERROR] Failed to save audio file: {audio_path}\")\n            return {\n                \"generated_audio_path\": None,\n                \"messages\": [AIMessage(content=\"Error: Failed to save audio file.\")]\n            }\n\n        return {\n            \"generated_audio_path\": audio_path,\n            \"messages\": [AIMessage(content=f\"Voiceover generated: {audio_path}\")]\n        }\n\n    except Exception as e:\n        import traceback\n        print(f\"[ERROR] TTS generation failed: {str(e)}\")\n        traceback.print_exc()\n        return {\n            \"generated_audio_path\": None,\n            \"messages\": [AIMessage(content=f\"TTS error: {str(e)}\")]\n        }"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Tool 4: Generate Video (LTX-Video)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def generate_video(state: AgentState) -> dict:\n    \"\"\"\n    Generates a promotional video using pre-loaded LTX-Video.\n    Uses the global video_pipe loaded during initialization.\n    LTX-Video is optimized for fast, real-time video generation.\n    \"\"\"\n    global video_pipe\n    from diffusers.utils import export_to_video\n\n    prompt = state.get(\"expanded_video_prompt\")\n    iteration = state.get(\"iteration_count\", 0)\n\n    if not prompt:\n        print(\"[ERROR] No video prompt available.\")\n        return {\"messages\": [AIMessage(content=\"Error: No video prompt available.\")]}\n\n    print(f\"\\n[VIDEO] Using LTX-Video (fast real-time generation)...\")\n    print(f\"  {get_gpu_memory_usage()}\")\n\n    try:\n        # Check if pipeline is loaded\n        if video_pipe is None:\n            print(\"  [VIDEO] Pipeline not loaded, loading now...\")\n            from diffusers import LTXPipeline\n            video_pipe = LTXPipeline.from_pretrained(\n                VIDEO_MODEL,\n                torch_dtype=torch.bfloat16\n            )\n\n        # Move to GPU\n        video_pipe.to(\"cuda\")\n        video_pipe.vae.enable_tiling()  # Memory optimization\n        print(f\"  [VIDEO] Model moved to GPU\")\n        print(f\"  {get_gpu_memory_usage()}\")\n\n        print(f\"[VIDEO] Generating video with LTX-Video...\")\n        print(f\"  Resolution: {VIDEO_WIDTH}x{VIDEO_HEIGHT}\")\n        print(f\"  Frames: {VIDEO_FRAMES} (~{VIDEO_FRAMES/VIDEO_FPS:.1f} seconds at {VIDEO_FPS} FPS)\")\n        print(f\"  Prompt: {prompt[:150]}...\")\n\n        # Generate video - LTX-Video is much faster than CogVideoX\n        start_time = time.time()\n\n        negative_prompt = \"worst quality, inconsistent motion, blurry, jittery, distorted\"\n\n        result = video_pipe(\n            prompt=prompt,\n            negative_prompt=negative_prompt,\n            width=VIDEO_WIDTH,\n            height=VIDEO_HEIGHT,\n            num_frames=VIDEO_FRAMES,\n            num_inference_steps=50,\n            decode_timestep=0.03,\n            decode_noise_scale=0.025,\n            generator=torch.Generator(device=\"cuda\").manual_seed(42)\n        )\n\n        elapsed = time.time() - start_time\n        print(f\"  [VIDEO] Generation took {elapsed:.1f} seconds\")\n\n        # Export to video file\n        video_frames = result.frames[0]\n        video_path = f\"promotional_video_v{iteration + 1}.mp4\"\n        export_to_video(video_frames, video_path, fps=VIDEO_FPS)\n\n        # Move back to CPU to free GPU memory\n        video_pipe.to(\"cpu\")\n        clear_gpu_memory()\n\n        # Verify file\n        import os\n        if os.path.exists(video_path):\n            file_size = os.path.getsize(video_path)\n            print(f\"[VIDEO] SUCCESS! Video saved: {video_path} ({file_size} bytes)\")\n        else:\n            print(f\"[ERROR] Failed to save video file\")\n\n        return {\n            \"generated_video_path\": video_path,\n            \"messages\": [AIMessage(content=f\"Video generated: {video_path}\")]\n        }\n\n    except Exception as e:\n        import traceback\n        if video_pipe is not None:\n            try:\n                video_pipe.to(\"cpu\")\n            except:\n                pass\n        clear_gpu_memory()\n        print(f\"[ERROR] Video generation failed: {str(e)}\")\n        traceback.print_exc()\n        return {\"messages\": [AIMessage(content=f\"Video error: {str(e)}\")]}"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool 5: Combine Video and Audio (MoviePy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def combine_video_audio(state: AgentState) -> dict:\n    \"\"\"\n    Combines generated video with voice-over audio using MoviePy.\n    Compatible with MoviePy 2.0+\n    Saves both the raw video and the combined video for debugging.\n    \"\"\"\n    import os\n\n    # MoviePy 2.0+ uses direct imports (not moviepy.editor)\n    try:\n        from moviepy import VideoFileClip, AudioFileClip\n        moviepy_version = \"2.0+\"\n    except ImportError:\n        # Fallback for older MoviePy versions\n        from moviepy.editor import VideoFileClip, AudioFileClip\n        moviepy_version = \"1.x\"\n\n    video_path = state.get(\"generated_video_path\")\n    audio_path = state.get(\"generated_audio_path\")\n    iteration = state.get(\"iteration_count\", 0)\n    include_vo = state.get(\"include_voiceover\", False)\n\n    print(f\"\\n[DEBUG] combine_video_audio called:\")\n    print(f\"  - MoviePy version: {moviepy_version}\")\n    print(f\"  - video_path: {video_path}\")\n    print(f\"  - audio_path: {audio_path}\")\n    print(f\"  - include_voiceover: {include_vo}\")\n\n    if not video_path:\n        print(\"[ERROR] No video path provided.\")\n        return {\n            \"final_video_path\": None,\n            \"messages\": [AIMessage(content=\"Error: No video to combine.\")]\n        }\n\n    # Check if video file exists\n    if not os.path.exists(video_path):\n        print(f\"[ERROR] Video file does not exist: {video_path}\")\n        return {\n            \"final_video_path\": None,\n            \"messages\": [AIMessage(content=f\"Error: Video file not found: {video_path}\")]\n        }\n\n    print(f\"[COMBINE] Video file exists: {video_path} ({os.path.getsize(video_path)} bytes)\")\n\n    # Check if we should skip audio combination\n    if not include_vo:\n        print(\"[INFO] Voiceover not requested, using video as-is.\")\n        return {\n            \"final_video_path\": video_path,\n            \"messages\": [AIMessage(content=\"Video ready (voiceover not requested).\")]\n        }\n\n    if not audio_path:\n        print(\"[WARNING] No audio path provided, using video without voiceover.\")\n        return {\n            \"final_video_path\": video_path,\n            \"messages\": [AIMessage(content=\"Video ready (no audio path).\")]\n        }\n\n    # Check if audio file exists\n    if not os.path.exists(audio_path):\n        print(f\"[ERROR] Audio file does not exist: {audio_path}\")\n        return {\n            \"final_video_path\": video_path,\n            \"messages\": [AIMessage(content=f\"Audio file not found, using video without audio.\")]\n        }\n\n    print(f\"[COMBINE] Audio file exists: {audio_path} ({os.path.getsize(audio_path)} bytes)\")\n\n    print(\"\\n[COMBINE] Starting video and audio combination...\")\n\n    try:\n        # Load video and audio\n        print(f\"[COMBINE] Loading video: {video_path}\")\n        video_clip = VideoFileClip(video_path)\n        print(f\"[COMBINE] Video loaded - duration: {video_clip.duration:.2f}s, size: {video_clip.size}\")\n\n        print(f\"[COMBINE] Loading audio: {audio_path}\")\n        audio_clip = AudioFileClip(audio_path)\n        print(f\"[COMBINE] Audio loaded - duration: {audio_clip.duration:.2f}s\")\n\n        # Adjust audio duration to match video if needed\n        if audio_clip.duration > video_clip.duration:\n            print(f\"[COMBINE] Trimming audio from {audio_clip.duration:.2f}s to {video_clip.duration:.2f}s\")\n            try:\n                # MoviePy 2.0+ uses subclipped()\n                audio_clip = audio_clip.subclipped(0, video_clip.duration)\n            except AttributeError:\n                # Fallback for older MoviePy versions\n                audio_clip = audio_clip.subclip(0, video_clip.duration)\n\n        # Set audio to video\n        print(\"[COMBINE] Combining video with audio...\")\n        try:\n            final_clip = video_clip.with_audio(audio_clip)\n        except AttributeError:\n            # Fallback for older MoviePy\n            final_clip = video_clip.set_audio(audio_clip)\n\n        # Export final video\n        final_path = f\"final_video_with_voiceover_v{iteration + 1}.mp4\"\n        print(f\"[COMBINE] Exporting to: {final_path}\")\n\n        # MoviePy 2.0+ doesn't support verbose/logger args\n        try:\n            # Try MoviePy 2.0+ style first\n            final_clip.write_videofile(\n                final_path,\n                codec='libx264',\n                audio_codec='aac'\n            )\n        except TypeError:\n            # Fallback for older MoviePy\n            final_clip.write_videofile(\n                final_path,\n                codec='libx264',\n                audio_codec='aac',\n                verbose=False,\n                logger=None\n            )\n\n        # Cleanup\n        video_clip.close()\n        audio_clip.close()\n        final_clip.close()\n\n        # Verify final file\n        if os.path.exists(final_path):\n            file_size = os.path.getsize(final_path)\n            print(f\"[COMBINE] SUCCESS! Final video saved: {final_path} ({file_size} bytes)\")\n        else:\n            print(f\"[ERROR] Final video file not created!\")\n            return {\n                \"final_video_path\": video_path,\n                \"messages\": [AIMessage(content=\"Error: Failed to create combined video.\")]\n            }\n\n        return {\n            \"final_video_path\": final_path,\n            \"messages\": [AIMessage(content=f\"Final video created: {final_path}\")]\n        }\n\n    except Exception as e:\n        import traceback\n        print(f\"[ERROR] Video/audio combination failed: {str(e)}\")\n        traceback.print_exc()\n        # Fall back to video without audio\n        return {\n            \"final_video_path\": video_path,\n            \"messages\": [AIMessage(content=f\"Combine error, using video without audio: {str(e)}\")]\n        }"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Tool 6: Modify Prompt (Qwen2.5-7B Local)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def modify_prompt(state: AgentState) -> dict:\n    \"\"\"\n    Modifies existing prompts based on user feedback using Qwen2.5-7B (local).\n    \"\"\"\n    feedback = state.get(\"feedback\", \"\")\n    current_image_prompt = state.get(\"expanded_image_prompt\", \"\")\n    current_video_prompt = state.get(\"expanded_video_prompt\", \"\")\n    current_voiceover = state.get(\"expanded_voiceover_script\", \"\")\n\n    system_prompt = f\"\"\"You are an expert at refining AI generation prompts based on user feedback.\n\nCurrent image prompt:\n{current_image_prompt}\n\nCurrent video prompt:\n{current_video_prompt}\n\nCurrent voiceover script:\n{current_voiceover}\n\nUser feedback: {feedback}\n\nModify the prompts to incorporate this feedback while preserving the core concept.\n\nRULES:\n- IMAGE: Do NOT request text rendering in the image\n- VIDEO: Do NOT include any people, humans, faces, hands in the prompt\n- VOICEOVER: Keep it under 40 words, natural and conversational\n\nFormat your response EXACTLY as:\nIMAGE_PROMPT: [modified image prompt]\nVIDEO_PROMPT: [modified video prompt - NO PEOPLE]\nVOICEOVER_SCRIPT: [modified script]\n\"\"\"\n\n    print(f\"\\nModifying prompts based on feedback: '{feedback}'\")\n\n    try:\n        messages = [\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": \"Please modify the prompts based on the feedback.\"}\n        ]\n\n        response_text = generate_llm_response(messages)\n        image_prompt, video_prompt, voiceover_script = parse_expanded_prompts(response_text)\n\n        # Use original if parsing fails\n        if not image_prompt:\n            image_prompt = current_image_prompt\n        if not video_prompt:\n            video_prompt = current_video_prompt\n        if not voiceover_script:\n            voiceover_script = current_voiceover\n\n        print(\"Prompts updated successfully.\")\n\n        return {\n            \"expanded_image_prompt\": image_prompt,\n            \"expanded_video_prompt\": video_prompt,\n            \"expanded_voiceover_script\": voiceover_script,\n            \"iteration_count\": state.get(\"iteration_count\", 0) + 1,\n            \"messages\": [AIMessage(content=\"Prompts updated based on feedback.\")]\n        }\n\n    except Exception as e:\n        print(f\"Error modifying prompts: {str(e)}\")\n        return {\n            \"iteration_count\": state.get(\"iteration_count\", 0) + 1,\n            \"messages\": [AIMessage(content=f\"Modify error: {str(e)}\")]\n        }"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def get_user_input_node(state: AgentState) -> dict:\n    \"\"\"Initial node to get user's creative brief.\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"   MARKETING CONTENT GENERATION AGENT (100% Open-Source)\")\n    print(\"   Powered by Qwen2.5, FLUX.1-dev, LTX-Video & Kokoro\")\n    print(\"   State-of-the-art quality - No API keys required!\")\n    print(\"=\"*60)\n\n    user_input = input(\"\\nDescribe what promotional material you need\\n(e.g., 'coffee shop promotional material'): \").strip()\n\n    if not user_input:\n        user_input = \"general business promotional material\"\n\n    return {\n        \"user_input\": user_input,\n        \"iteration_count\": 0,\n        \"is_satisfied\": False,\n        \"include_voiceover\": False,\n        \"voice_style\": DEFAULT_VOICE,\n        \"messages\": [HumanMessage(content=user_input)]\n    }\n\n\ndef expand_prompt_node(state: AgentState) -> dict:\n    \"\"\"Node that calls the expand_prompt tool.\"\"\"\n    return expand_prompt(state)\n\n\ndef ask_generation_mode_node(state: AgentState) -> dict:\n    \"\"\"Ask user what type of content to generate, including voiceover option.\"\"\"\n    print(\"\\n\" + \"-\"*50)\n    print(\"GENERATION OPTIONS\")\n    print(\"-\"*50)\n    print(\"\\nWhat would you like to generate?\")\n    print(\"  1. Image only (FLUX.1-dev)\")\n    print(\"  2. Video only (LTX-Video, no voiceover)\")\n    print(\"  3. Video with voice-over (LTX-Video + Kokoro)\")\n    print(\"  4. Both image and video (with voice-over)\")\n    print(\"  5. Both image and video (no voice-over)\")\n\n    choice = input(\"\\nEnter your choice (1-5): \").strip()\n\n    mode_map = {\n        \"1\": (\"image\", False),\n        \"2\": (\"video\", False),\n        \"3\": (\"video\", True),\n        \"4\": (\"both\", True),\n        \"5\": (\"both\", False)\n    }\n    mode, include_vo = mode_map.get(choice, (\"both\", True))\n\n    print(f\"\\nSelected: {mode}\" + (\" with voiceover\" if include_vo else \"\"))\n\n    return {\n        \"generation_mode\": mode,\n        \"include_voiceover\": include_vo,\n        \"messages\": [HumanMessage(content=f\"Generate: {mode}, Voiceover: {include_vo}\")]\n    }"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(img, title=\"Generated Image\"):\n",
    "    \"\"\"Display PIL image in notebook.\"\"\"\n",
    "    print(f\"\\n{title}:\")\n",
    "    try:\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not display image: {e}\")\n",
    "        print(\"Image saved to file.\")\n",
    "\n",
    "\n",
    "def display_video_in_notebook(video_path):\n",
    "    \"\"\"Display video in Jupyter/Colab notebook.\"\"\"\n",
    "    try:\n",
    "        from IPython.display import Video, display\n",
    "        display(Video(video_path, embed=True, height=400))\n",
    "    except Exception as e:\n",
    "        print(f\"Could not display video inline: {e}\")\n",
    "        print(f\"Video saved at: {video_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def generate_content_node(state: AgentState) -> dict:\n    \"\"\"\n    Main generation node that routes to appropriate tools.\n    Properly passes state through the pipeline with debugging.\n    \"\"\"\n    import os\n\n    mode = state.get(\"generation_mode\", \"both\")\n    include_vo = state.get(\"include_voiceover\", False)\n    results = {}\n\n    print(\"\\n\" + \"=\"*60)\n    print(\"GENERATING CONTENT\")\n    print(\"=\"*60)\n    print(f\"\\n[STATE DEBUG]\")\n    print(f\"  - generation_mode: {mode}\")\n    print(f\"  - include_voiceover: {include_vo}\")\n    print(f\"  - voice_style: {state.get('voice_style', 'not set')}\")\n    print(f\"  - voiceover_script: {state.get('expanded_voiceover_script', 'not set')[:50] if state.get('expanded_voiceover_script') else 'None'}...\")\n\n    # Step 1: Generate image if requested\n    if mode in [\"image\", \"both\"]:\n        print(\"\\n\" + \"-\"*50)\n        print(\"[Step 1/4] GENERATING IMAGE\")\n        print(\"-\"*50)\n        img_result = generate_poster(state)\n        results.update(img_result)\n        if img_result.get(\"generated_image\"):\n            display_image(img_result[\"generated_image\"], \"Generated Promotional Image\")\n\n    # Step 2: Generate voiceover if requested (before video to save memory)\n    if mode in [\"video\", \"both\"] and include_vo:\n        print(\"\\n\" + \"-\"*50)\n        print(\"[Step 2/4] GENERATING VOICEOVER\")\n        print(\"-\"*50)\n        # Create combined state with all values\n        vo_state = {**state, **results}\n        vo_result = generate_voiceover(vo_state)\n        results.update(vo_result)\n\n        # Debug: Check if audio was generated\n        audio_path = vo_result.get(\"generated_audio_path\")\n        if audio_path and os.path.exists(audio_path):\n            print(f\"\\n[CHECKPOINT] Voiceover file verified: {audio_path}\")\n        else:\n            print(f\"\\n[WARNING] Voiceover may not have been generated properly\")\n    elif mode in [\"video\", \"both\"]:\n        print(\"\\n[Step 2/4] SKIPPING VOICEOVER (not requested)\")\n\n    # Step 3: Generate video if requested\n    if mode in [\"video\", \"both\"]:\n        print(\"\\n\" + \"-\"*50)\n        print(\"[Step 3/4] GENERATING VIDEO\")\n        print(\"-\"*50)\n        vid_state = {**state, **results}\n        vid_result = generate_video(vid_state)\n        results.update(vid_result)\n\n        # Debug: Check if video was generated\n        video_path = vid_result.get(\"generated_video_path\")\n        if video_path and os.path.exists(video_path):\n            print(f\"\\n[CHECKPOINT] Video file verified: {video_path}\")\n\n        # Step 4: Combine video and audio\n        print(\"\\n\" + \"-\"*50)\n        print(\"[Step 4/4] COMBINING VIDEO AND AUDIO\")\n        print(\"-\"*50)\n\n        # Create combined state with ALL accumulated results\n        combine_state = {**state, **results}\n\n        # Debug: Show what we're passing to combine\n        print(f\"\\n[DEBUG] State being passed to combine_video_audio:\")\n        print(f\"  - generated_video_path: {combine_state.get('generated_video_path')}\")\n        print(f\"  - generated_audio_path: {combine_state.get('generated_audio_path')}\")\n        print(f\"  - include_voiceover: {combine_state.get('include_voiceover')}\")\n\n        combine_result = combine_video_audio(combine_state)\n        results.update(combine_result)\n\n        # Display final video\n        final_path = combine_result.get(\"final_video_path\")\n        if final_path and os.path.exists(final_path):\n            print(f\"\\n[SUCCESS] Displaying final video: {final_path}\")\n            display_video_in_notebook(final_path)\n        elif video_path:\n            print(f\"\\n[INFO] Displaying raw video (no voiceover): {video_path}\")\n            display_video_in_notebook(video_path)\n\n    # Print summary of generated files\n    print(\"\\n\" + \"=\"*60)\n    print(\"GENERATION SUMMARY\")\n    print(\"=\"*60)\n    print(\"\\nGenerated files:\")\n    for key, value in results.items():\n        if 'path' in key.lower() and value:\n            if os.path.exists(value):\n                size = os.path.getsize(value)\n                print(f\"  - {key}: {value} ({size} bytes)\")\n            else:\n                print(f\"  - {key}: {value} (FILE NOT FOUND!)\")\n\n    return results\n\n\ndef collect_feedback_node(state: AgentState) -> dict:\n    \"\"\"Collect user feedback on generated content.\"\"\"\n    iteration = state.get(\"iteration_count\", 0)\n\n    # Check iteration limit\n    if iteration >= MAX_ITERATIONS:\n        print(f\"\\nMaximum iterations ({MAX_ITERATIONS}) reached.\")\n        return {\n            \"is_satisfied\": True,\n            \"messages\": [AIMessage(content=\"Max iterations reached.\")]\n        }\n\n    print(\"\\n\" + \"-\"*50)\n    print(\"FEEDBACK\")\n    print(\"-\"*50)\n    print(\"\\nAre you satisfied with the generated content?\")\n    print(\"  - Type 'yes' or 'done' if satisfied\")\n    print(\"  - Or provide feedback (e.g., 'make it more colorful', 'add more energy')\")\n\n    feedback = input(\"\\nYour response: \").strip()\n\n    if feedback.lower() in [\"yes\", \"done\", \"satisfied\", \"looks good\", \"perfect\", \"ok\", \"okay\", \"y\"]:\n        return {\n            \"is_satisfied\": True,\n            \"messages\": [HumanMessage(content=\"Satisfied.\")]\n        }\n    else:\n        return {\n            \"feedback\": feedback,\n            \"is_satisfied\": False,\n            \"messages\": [HumanMessage(content=f\"Feedback: {feedback}\")]\n        }\n\n\ndef modify_prompt_node(state: AgentState) -> dict:\n    \"\"\"Node that calls the modify_prompt tool.\"\"\"\n    return modify_prompt(state)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def build_agent_graph() -> StateGraph:\n    \"\"\"\n    Builds the LangGraph agent with all nodes and edges.\n\n    Graph Structure:\n\n    START --> get_user_input --> expand_prompt --> ask_generation_mode\n                                                         |\n                                                         v\n                                              generate_content\n                                              (image -> voiceover -> video -> combine)\n                                                         |\n                                                         v\n                                              collect_feedback\n                                                    /    \\\n                                           [satisfied]  [feedback]\n                                                /            \\\n                                              END        modify_prompt\n                                                              |\n                                                              v\n                                                    generate_content (loop)\n    \"\"\"\n    graph = StateGraph(AgentState)\n\n    # Add all nodes\n    graph.add_node(\"get_user_input\", get_user_input_node)\n    graph.add_node(\"expand_prompt\", expand_prompt_node)\n    graph.add_node(\"ask_generation_mode\", ask_generation_mode_node)\n    graph.add_node(\"generate_content\", generate_content_node)\n    graph.add_node(\"collect_feedback\", collect_feedback_node)\n    graph.add_node(\"modify_prompt\", modify_prompt_node)\n\n    # Add edges - linear flow\n    graph.add_edge(START, \"get_user_input\")\n    graph.add_edge(\"get_user_input\", \"expand_prompt\")\n    graph.add_edge(\"expand_prompt\", \"ask_generation_mode\")\n    graph.add_edge(\"ask_generation_mode\", \"generate_content\")\n    graph.add_edge(\"generate_content\", \"collect_feedback\")\n\n    # Conditional edge for feedback loop\n    def route_after_feedback(state: AgentState) -> str:\n        if state.get(\"is_satisfied\", False):\n            return \"end\"\n        else:\n            return \"modify\"\n\n    graph.add_conditional_edges(\n        \"collect_feedback\",\n        route_after_feedback,\n        {\"end\": END, \"modify\": \"modify_prompt\"}\n    )\n\n    # Loop back after modification\n    graph.add_edge(\"modify_prompt\", \"generate_content\")\n\n    return graph.compile()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def run_agent():\n    \"\"\"Main entry point for the marketing content generation agent.\"\"\"\n    import os\n\n    # Build and compile the graph\n    agent = build_agent_graph()\n\n    # Initialize empty state\n    initial_state = {\n        \"messages\": [],\n        \"user_input\": \"\",\n        \"feedback\": None,\n        \"expanded_image_prompt\": None,\n        \"expanded_video_prompt\": None,\n        \"expanded_voiceover_script\": None,\n        \"generation_mode\": None,\n        \"include_voiceover\": False,\n        \"voice_style\": DEFAULT_VOICE,\n        \"generated_image\": None,\n        \"generated_video_path\": None,\n        \"generated_audio_path\": None,\n        \"final_video_path\": None,\n        \"iteration_count\": 0,\n        \"is_satisfied\": False,\n    }\n\n    try:\n        # Run the agent graph\n        final_state = agent.invoke(initial_state)\n\n        # Final Summary\n        print(\"\\n\" + \"=\"*60)\n        print(\"   GENERATION COMPLETE\")\n        print(\"=\"*60)\n        print(f\"\\nTotal iterations: {final_state.get('iteration_count', 0) + 1}\")\n\n        # List all generated files\n        print(\"\\n\" + \"-\"*40)\n        print(\"GENERATED FILES:\")\n        print(\"-\"*40)\n\n        files_generated = []\n\n        # Check for image\n        if final_state.get(\"generated_image\"):\n            iteration = final_state.get(\"iteration_count\", 0)\n            img_path = f\"promotional_image_v{iteration + 1}.png\"\n            if os.path.exists(img_path):\n                files_generated.append((\"Image\", img_path, os.path.getsize(img_path)))\n\n        # Check for voiceover audio\n        audio_path = final_state.get(\"generated_audio_path\")\n        if audio_path and os.path.exists(audio_path):\n            files_generated.append((\"Voiceover (WAV)\", audio_path, os.path.getsize(audio_path)))\n\n        # Check for raw video\n        video_path = final_state.get(\"generated_video_path\")\n        if video_path and os.path.exists(video_path):\n            files_generated.append((\"Raw Video (no audio)\", video_path, os.path.getsize(video_path)))\n\n        # Check for final video with voiceover\n        final_path = final_state.get(\"final_video_path\")\n        if final_path and os.path.exists(final_path) and final_path != video_path:\n            files_generated.append((\"Final Video (with voiceover)\", final_path, os.path.getsize(final_path)))\n\n        if files_generated:\n            for file_type, file_path, file_size in files_generated:\n                size_str = f\"{file_size / 1024:.1f} KB\" if file_size < 1024*1024 else f\"{file_size / (1024*1024):.1f} MB\"\n                print(f\"  [{file_type}]\")\n                print(f\"    Path: {file_path}\")\n                print(f\"    Size: {size_str}\")\n                print()\n        else:\n            print(\"  No files were generated.\")\n\n        print(\"-\"*40)\n        print(\"\\nThank you for using the Marketing Content Generator!\")\n        print(\"All files are saved in the current directory.\")\n\n        return final_state\n\n    except KeyboardInterrupt:\n        print(\"\\n\\nAgent interrupted by user.\")\n        return None\n    except Exception as e:\n        print(f\"\\n\\nError: {str(e)}\")\n        import traceback\n        traceback.print_exc()\n        return None"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the agent\n",
    "final_state = run_agent()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}