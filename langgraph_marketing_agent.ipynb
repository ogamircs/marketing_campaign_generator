{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marketing Campaign Creatives Generation Agent\n",
    "\n",
    "Interactive LangGraph-based AI Agent for Marketing Content Generation.\n",
    "\n",
    "This notebook implements an intelligent agent that:\n",
    "- Takes short user inputs and expands them into detailed prompts\n",
    "- Generates promotional images using Imagen 3.0\n",
    "- Generates promotional videos using Veo 3.1\n",
    "- Allows iterative feedback and refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing the Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U google-genai langgraph langchain-core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** After running the above cell, kindly restart the runtime and run all cells sequentially from the next cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Generative AI Libraries\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# LangGraph Libraries\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "# LangChain Core\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "\n",
    "# Type Annotations\n",
    "from typing import TypedDict, Annotated, Literal, Optional, List, Any\n",
    "\n",
    "# Utility Libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authentication and Client Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate the user for Google Cloud services\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Google Cloud project details\n",
    "PROJECT_ID = \"your-project-code\"  # @param {type: \"string\"}\n",
    "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")\n",
    "\n",
    "# Initialize the client for GenAI with project and location details\n",
    "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "# Model identifiers\n",
    "IMAGE_MODEL = \"imagen-3.0-generate-001\"\n",
    "VIDEO_MODEL = \"veo-3.1-fast-generate-001\"\n",
    "GEMINI_MODEL = \"gemini-2.0-flash\"\n",
    "\n",
    "# Configuration constants\n",
    "MAX_ITERATIONS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent State Schema Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    State schema for the marketing content generation agent.\n",
    "    \"\"\"\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    user_input: str\n",
    "    feedback: Optional[str]\n",
    "    expanded_image_prompt: Optional[str]\n",
    "    expanded_video_prompt: Optional[str]\n",
    "    generation_mode: Optional[Literal[\"image\", \"video\", \"both\"]]\n",
    "    generated_image: Optional[Any]\n",
    "    generated_video_path: Optional[str]\n",
    "    iteration_count: int\n",
    "    is_satisfied: bool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_prompt(state: AgentState) -> dict:\n",
    "    \"\"\"\n",
    "    Uses Gemini to expand a short user input into detailed image and video prompts.\n",
    "    \"\"\"\n",
    "    user_input = state[\"user_input\"]\n",
    "\n",
    "    system_prompt = \"\"\"You are an expert at creating detailed prompts for AI image and video generation.\n",
    "Given a short description of promotional material needed, create TWO detailed prompts:\n",
    "\n",
    "1. IMAGE PROMPT: A detailed description for generating a promotional poster/image.\n",
    "   Include: visual style, colors, composition, text elements, mood, and specific details.\n",
    "\n",
    "2. VIDEO PROMPT: A detailed description for generating an 8-second promotional video.\n",
    "   Include: scene descriptions, camera movements, pacing, mood, and any text overlays.\n",
    "   \n",
    "   CRITICAL RULE FOR VIDEO: Do NOT include any people, humans, faces, hands, baristas, customers, \n",
    "   or any human figures in the video prompt. Focus ONLY on:\n",
    "   - Products (food, drinks, items)\n",
    "   - Environment/atmosphere (interior, exterior, decor)\n",
    "   - Objects and textures (cups, tables, steam, etc.)\n",
    "   - Abstract visuals and motion graphics\n",
    "   - Text overlays and branding elements\n",
    "\n",
    "Format your response EXACTLY as:\n",
    "IMAGE_PROMPT: [detailed image prompt here]\n",
    "VIDEO_PROMPT: [detailed video prompt here - NO PEOPLE]\n",
    "\"\"\"\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=GEMINI_MODEL,\n",
    "        contents=[{\"role\": \"user\", \"parts\": [{\"text\": f\"{system_prompt}\\n\\nUser request: {user_input}\"}]}]\n",
    "    )\n",
    "\n",
    "    response_text = response.text\n",
    "    image_prompt = \"\"\n",
    "    video_prompt = \"\"\n",
    "\n",
    "    if \"IMAGE_PROMPT:\" in response_text and \"VIDEO_PROMPT:\" in response_text:\n",
    "        parts = response_text.split(\"VIDEO_PROMPT:\")\n",
    "        image_prompt = parts[0].replace(\"IMAGE_PROMPT:\", \"\").strip()\n",
    "        if len(parts) > 1:\n",
    "            video_prompt = parts[1].strip()\n",
    "\n",
    "    print(\"\\n\" + \"-\"*50)\n",
    "    print(\"EXPANDED PROMPTS\")\n",
    "    print(\"-\"*50)\n",
    "    print(f\"\\nImage Prompt Preview:\\n{image_prompt[:300]}...\")\n",
    "    print(f\"\\nVideo Prompt Preview:\\n{video_prompt[:300]}...\")\n",
    "\n",
    "    return {\n",
    "        \"expanded_image_prompt\": image_prompt,\n",
    "        \"expanded_video_prompt\": video_prompt,\n",
    "        \"messages\": [AIMessage(content=\"Expanded your request into detailed prompts.\")]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_poster(state: AgentState) -> dict:\n",
    "    \"\"\"\n",
    "    Generates a promotional image using Imagen 3.0.\n",
    "    \"\"\"\n",
    "    prompt = state.get(\"expanded_image_prompt\")\n",
    "\n",
    "    if not prompt:\n",
    "        print(\"Error: No image prompt available.\")\n",
    "        return {\"messages\": [AIMessage(content=\"Error: No image prompt available.\")]}\n",
    "\n",
    "    print(\"\\nGenerating image with Imagen 3.0...\")\n",
    "\n",
    "    try:\n",
    "        response = client.models.generate_images(\n",
    "            model=IMAGE_MODEL,\n",
    "            prompt=prompt,\n",
    "            config=types.GenerateImagesConfig(number_of_images=1)\n",
    "        )\n",
    "        img = response.generated_images[0].image\n",
    "        print(\"Image generated successfully!\")\n",
    "        return {\"generated_image\": img, \"messages\": [AIMessage(content=\"Image generated!\")]}\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating image: {str(e)}\")\n",
    "        return {\"messages\": [AIMessage(content=f\"Error: {str(e)}\")]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_video(state: AgentState) -> dict:\n",
    "    \"\"\"\n",
    "    Generates a promotional video using Veo 3.1 with async polling.\n",
    "    \"\"\"\n",
    "    prompt = state.get(\"expanded_video_prompt\")\n",
    "    iteration = state.get(\"iteration_count\", 0)\n",
    "\n",
    "    if not prompt:\n",
    "        print(\"Error: No video prompt available.\")\n",
    "        return {\"messages\": [AIMessage(content=\"Error: No video prompt available.\")]}\n",
    "\n",
    "    print(\"\\nGenerating video with Veo 3.1... (this may take a few minutes)\")\n",
    "    print(f\"Video prompt: {prompt[:200]}...\")\n",
    "\n",
    "    try:\n",
    "        operation = client.models.generate_videos(\n",
    "            model=VIDEO_MODEL,\n",
    "            prompt=prompt,\n",
    "            config=types.GenerateVideosConfig(\n",
    "                aspect_ratio=\"9:16\",\n",
    "                number_of_videos=1,\n",
    "                duration_seconds=8,\n",
    "                person_generation=\"dont_allow\",\n",
    "                enhance_prompt=True,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        while not operation.done:\n",
    "            time.sleep(15)\n",
    "            operation = client.operations.get(operation)\n",
    "            print(f\"  Status: {operation}\")\n",
    "\n",
    "        # Debug: print full operation details\n",
    "        print(f\"\\nOperation completed. Response: {operation.response}\")\n",
    "        \n",
    "        # Check if operation completed successfully (matching original code pattern)\n",
    "        if operation.response:\n",
    "            try:\n",
    "                generated_video = operation.result.generated_videos[0].video\n",
    "                video_filename = f\"promotional_video_v{iteration + 1}.mp4\"\n",
    "                generated_video.save(video_filename)\n",
    "                print(f\"Video saved as: {video_filename}\")\n",
    "                return {\"generated_video_path\": video_filename, \"messages\": [AIMessage(content=f\"Video saved: {video_filename}\")]}\n",
    "            except (IndexError, AttributeError, TypeError) as e:\n",
    "                # Check for rai_media_filtered_reasons (content policy)\n",
    "                if hasattr(operation.result, 'rai_media_filtered_reasons'):\n",
    "                    reasons = operation.result.rai_media_filtered_reasons\n",
    "                    print(f\"Video filtered by content policy: {reasons}\")\n",
    "                    return {\"messages\": [AIMessage(content=f\"Video blocked by content policy: {reasons}\")]}\n",
    "                else:\n",
    "                    print(f\"Could not extract video: {e}\")\n",
    "                    print(f\"Operation result: {operation.result}\")\n",
    "                    return {\"messages\": [AIMessage(content=f\"Video extraction failed: {e}\")]}\n",
    "        else:\n",
    "            print(f\"No response. Operation: {operation}\")\n",
    "            return {\"messages\": [AIMessage(content=\"Video generation returned no response.\")]}\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating video: {str(e)}\")\n",
    "        return {\"messages\": [AIMessage(content=f\"Error: {str(e)}\")]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_prompt(state: AgentState) -> dict:\n",
    "    \"\"\"\n",
    "    Takes user feedback and modifies existing prompts accordingly.\n",
    "    \"\"\"\n",
    "    feedback = state.get(\"feedback\", \"\")\n",
    "    current_image_prompt = state.get(\"expanded_image_prompt\", \"\")\n",
    "    current_video_prompt = state.get(\"expanded_video_prompt\", \"\")\n",
    "    generation_mode = state.get(\"generation_mode\", \"both\")\n",
    "\n",
    "    system_prompt = f\"\"\"You are an expert at refining AI generation prompts based on user feedback.\n",
    "\n",
    "Current image prompt:\n",
    "{current_image_prompt}\n",
    "\n",
    "Current video prompt:\n",
    "{current_video_prompt}\n",
    "\n",
    "User feedback: {feedback}\n",
    "\n",
    "Please modify the prompts to incorporate this feedback while preserving the core concept.\n",
    "\n",
    "CRITICAL RULE FOR VIDEO: Do NOT include any people, humans, faces, hands, baristas, customers,\n",
    "or any human figures in the video prompt. Focus ONLY on products, environment, objects, textures,\n",
    "abstract visuals, motion graphics, and text overlays.\n",
    "\n",
    "Format your response EXACTLY as:\n",
    "IMAGE_PROMPT: [modified image prompt]\n",
    "VIDEO_PROMPT: [modified video prompt - NO PEOPLE]\n",
    "\"\"\"\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=GEMINI_MODEL,\n",
    "        contents=[{\"role\": \"user\", \"parts\": [{\"text\": system_prompt}]}]\n",
    "    )\n",
    "\n",
    "    response_text = response.text\n",
    "    new_image_prompt = current_image_prompt\n",
    "    new_video_prompt = current_video_prompt\n",
    "\n",
    "    if \"IMAGE_PROMPT:\" in response_text and \"VIDEO_PROMPT:\" in response_text:\n",
    "        parts = response_text.split(\"VIDEO_PROMPT:\")\n",
    "        img_part = parts[0].replace(\"IMAGE_PROMPT:\", \"\").strip()\n",
    "        if img_part:\n",
    "            new_image_prompt = img_part\n",
    "        if len(parts) > 1:\n",
    "            vid_part = parts[1].strip()\n",
    "            if vid_part:\n",
    "                new_video_prompt = vid_part\n",
    "\n",
    "    print(f\"\\nPrompts updated based on feedback: '{feedback}'\")\n",
    "\n",
    "    return {\n",
    "        \"expanded_image_prompt\": new_image_prompt,\n",
    "        \"expanded_video_prompt\": new_video_prompt,\n",
    "        \"iteration_count\": state.get(\"iteration_count\", 0) + 1,\n",
    "        \"messages\": [AIMessage(content=\"Prompts updated.\")]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_input_node(state: AgentState) -> dict:\n",
    "    \"\"\"Initial node to get user's creative brief.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"   MARKETING CONTENT GENERATION AGENT\")\n",
    "    print(\"   Powered by LangGraph, Gemini, Imagen 3.0 & Veo 3.1\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    user_input = input(\"\\nDescribe what promotional material you need\\n(e.g., 'coffee shop promotional material'): \").strip()\n",
    "\n",
    "    if not user_input:\n",
    "        user_input = \"general business promotional material\"\n",
    "\n",
    "    return {\n",
    "        \"user_input\": user_input,\n",
    "        \"iteration_count\": 0,\n",
    "        \"is_satisfied\": False,\n",
    "        \"messages\": [HumanMessage(content=user_input)]\n",
    "    }\n",
    "\n",
    "\n",
    "def expand_prompt_node(state: AgentState) -> dict:\n",
    "    \"\"\"Node that calls the expand_prompt tool.\"\"\"\n",
    "    return expand_prompt(state)\n",
    "\n",
    "\n",
    "def ask_generation_mode_node(state: AgentState) -> dict:\n",
    "    \"\"\"Ask user what type of content to generate.\"\"\"\n",
    "    print(\"\\n\" + \"-\"*50)\n",
    "    print(\"GENERATION OPTIONS\")\n",
    "    print(\"-\"*50)\n",
    "    print(\"\\nWhat would you like to generate?\")\n",
    "    print(\"  1. Image only\")\n",
    "    print(\"  2. Video only\")\n",
    "    print(\"  3. Both image and video\")\n",
    "\n",
    "    choice = input(\"\\nEnter your choice (1/2/3): \").strip()\n",
    "    mode_map = {\"1\": \"image\", \"2\": \"video\", \"3\": \"both\"}\n",
    "    mode = mode_map.get(choice, \"both\")\n",
    "\n",
    "    print(f\"\\nSelected: {mode}\")\n",
    "    return {\"generation_mode\": mode, \"messages\": [HumanMessage(content=f\"Generate: {mode}\")]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(img, title=\"Generated Image\"):\n",
    "    \"\"\"Display image in Colab/Jupyter notebook.\"\"\"\n",
    "    print(f\"\\n{title}:\")\n",
    "    try:\n",
    "        # The Google GenAI image object has a show() method\n",
    "        img.show()\n",
    "    except Exception as e1:\n",
    "        try:\n",
    "            # Try saving to file (Google's image object has save method without format arg)\n",
    "            filename = \"generated_image.png\"\n",
    "            img.save(filename)\n",
    "            print(f\"Image saved as: {filename}\")\n",
    "            \n",
    "            # Display from file in Colab\n",
    "            from IPython.display import display, Image as IPImage\n",
    "            display(IPImage(filename=filename))\n",
    "        except Exception as e2:\n",
    "            print(f\"Could not display image: {e2}\")\n",
    "            print(\"Image object available in final_state['generated_image']\")\n",
    "\n",
    "\n",
    "def display_video_in_notebook(video_path):\n",
    "    \"\"\"Display video in Jupyter/Colab notebook.\"\"\"\n",
    "    try:\n",
    "        from IPython.display import Video, display\n",
    "        display(Video(video_path, embed=True, height=500))\n",
    "    except Exception as e:\n",
    "        print(f\"Could not display video inline: {e}\")\n",
    "        print(f\"Video saved at: {video_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_content_node(state: AgentState) -> dict:\n",
    "    \"\"\"Routes to appropriate generation based on mode.\"\"\"\n",
    "    mode = state.get(\"generation_mode\", \"both\")\n",
    "    results = {}\n",
    "\n",
    "    print(\"\\n\" + \"-\"*50)\n",
    "    print(\"GENERATING CONTENT\")\n",
    "    print(\"-\"*50)\n",
    "\n",
    "    if mode in [\"image\", \"both\"]:\n",
    "        img_result = generate_poster(state)\n",
    "        results.update(img_result)\n",
    "        if img_result.get(\"generated_image\"):\n",
    "            display_image(img_result[\"generated_image\"], \"Generated Promotional Image\")\n",
    "\n",
    "    if mode in [\"video\", \"both\"]:\n",
    "        vid_result = generate_video(state)\n",
    "        results.update(vid_result)\n",
    "        if vid_result.get(\"generated_video_path\"):\n",
    "            display_video_in_notebook(vid_result[\"generated_video_path\"])\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def collect_feedback_node(state: AgentState) -> dict:\n",
    "    \"\"\"Collect user feedback on generated content.\"\"\"\n",
    "    iteration = state.get(\"iteration_count\", 0)\n",
    "\n",
    "    if iteration >= MAX_ITERATIONS:\n",
    "        print(f\"\\nMaximum iterations ({MAX_ITERATIONS}) reached.\")\n",
    "        return {\"is_satisfied\": True, \"messages\": [AIMessage(content=\"Max iterations reached.\")]}\n",
    "\n",
    "    print(\"\\n\" + \"-\"*50)\n",
    "    print(\"FEEDBACK\")\n",
    "    print(\"-\"*50)\n",
    "    print(\"\\nAre you satisfied with the generated content?\")\n",
    "    print(\"  - Type 'yes' or 'done' if satisfied\")\n",
    "    print(\"  - Or provide feedback (e.g., 'make it more colorful')\")\n",
    "\n",
    "    feedback = input(\"\\nYour response: \").strip()\n",
    "\n",
    "    if feedback.lower() in [\"yes\", \"done\", \"satisfied\", \"looks good\", \"perfect\", \"ok\", \"okay\", \"y\"]:\n",
    "        return {\"is_satisfied\": True, \"messages\": [HumanMessage(content=\"Satisfied.\")]}\n",
    "    else:\n",
    "        return {\"feedback\": feedback, \"is_satisfied\": False, \"messages\": [HumanMessage(content=f\"Feedback: {feedback}\")]}\n",
    "\n",
    "\n",
    "def modify_prompt_node(state: AgentState) -> dict:\n",
    "    \"\"\"Node that calls the modify_prompt tool.\"\"\"\n",
    "    return modify_prompt(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent_graph() -> StateGraph:\n",
    "    \"\"\"\n",
    "    Builds the LangGraph agent with all nodes and edges.\n",
    "\n",
    "    Graph Structure:\n",
    "    START --> get_user_input --> expand_prompt --> ask_generation_mode\n",
    "                                                         |\n",
    "                                                         v\n",
    "                                              generate_content\n",
    "                                                         |\n",
    "                                                         v\n",
    "                                              collect_feedback\n",
    "                                                    /    \\\n",
    "                                           [satisfied]  [feedback]\n",
    "                                                /            \\\n",
    "                                              END        modify_prompt\n",
    "                                                              |\n",
    "                                                              v\n",
    "                                                    generate_content (loop)\n",
    "    \"\"\"\n",
    "    graph = StateGraph(AgentState)\n",
    "\n",
    "    # Add all nodes\n",
    "    graph.add_node(\"get_user_input\", get_user_input_node)\n",
    "    graph.add_node(\"expand_prompt\", expand_prompt_node)\n",
    "    graph.add_node(\"ask_generation_mode\", ask_generation_mode_node)\n",
    "    graph.add_node(\"generate_content\", generate_content_node)\n",
    "    graph.add_node(\"collect_feedback\", collect_feedback_node)\n",
    "    graph.add_node(\"modify_prompt\", modify_prompt_node)\n",
    "\n",
    "    # Add edges\n",
    "    graph.add_edge(START, \"get_user_input\")\n",
    "    graph.add_edge(\"get_user_input\", \"expand_prompt\")\n",
    "    graph.add_edge(\"expand_prompt\", \"ask_generation_mode\")\n",
    "    graph.add_edge(\"ask_generation_mode\", \"generate_content\")\n",
    "    graph.add_edge(\"generate_content\", \"collect_feedback\")\n",
    "\n",
    "    # Conditional edge for feedback loop\n",
    "    def route_after_feedback(state: AgentState) -> str:\n",
    "        if state.get(\"is_satisfied\", False):\n",
    "            return \"end\"\n",
    "        else:\n",
    "            return \"modify\"\n",
    "\n",
    "    graph.add_conditional_edges(\n",
    "        \"collect_feedback\",\n",
    "        route_after_feedback,\n",
    "        {\"end\": END, \"modify\": \"modify_prompt\"}\n",
    "    )\n",
    "\n",
    "    graph.add_edge(\"modify_prompt\", \"generate_content\")\n",
    "\n",
    "    return graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent():\n",
    "    \"\"\"Main entry point for the marketing content generation agent.\"\"\"\n",
    "    agent = build_agent_graph()\n",
    "\n",
    "    initial_state = {\n",
    "        \"messages\": [],\n",
    "        \"user_input\": \"\",\n",
    "        \"feedback\": None,\n",
    "        \"expanded_image_prompt\": None,\n",
    "        \"expanded_video_prompt\": None,\n",
    "        \"generation_mode\": None,\n",
    "        \"generated_image\": None,\n",
    "        \"generated_video_path\": None,\n",
    "        \"iteration_count\": 0,\n",
    "        \"is_satisfied\": False,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        final_state = agent.invoke(initial_state)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"   GENERATION COMPLETE\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"\\nTotal iterations: {final_state.get('iteration_count', 0) + 1}\")\n",
    "\n",
    "        if final_state.get(\"generated_image\"):\n",
    "            print(\"Image: Generated successfully\")\n",
    "        if final_state.get(\"generated_video_path\"):\n",
    "            print(f\"Video: {final_state['generated_video_path']}\")\n",
    "\n",
    "        print(\"\\nThank you for using the Marketing Content Generator!\")\n",
    "        return final_state\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nAgent interrupted.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"\\n\\nError: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the agent\n",
    "final_state = run_agent()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
